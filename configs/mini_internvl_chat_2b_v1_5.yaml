tokenizer:
  name: InternLM2Tokenizer
  vocab_file: "./MiniInternLM2Chat2B/tokenizer.model"

llm_model:
  name: InternLM2CausalLM
  batch_size: 1
  seq_length: 8192
  hidden_size: 2048
  num_layers: 24
  num_heads: 16
  n_kv_heads: 8
  vocab_size: 92553
  intermediate_size: 8192
  rms_norm_eps: 1.0e-6
  emb_dropout_prob: 0.0
  eos_token_id: 2
  pad_token_id: 2
  compute_dtype: "float16"
  layernorm_compute_type: "float32"
  softmax_compute_type: "float16"
  rotary_dtype: "float16"
  param_init_type: "float16"
  ln_param_init_type: "float16"
  use_past: True
  use_flash_attention: False
  use_past_shard: False
  offset: 0
  checkpoint_name_or_path: "/path/to/vary_toy.ckpt"
  repetition_penalty: 1.0
  max_decode_length: 2048
  top_k: 50
  top_p: 1.0
  do_sample: False
  max_new_tokens: 1024
  temperature: 1.0
  num_beams: 1

  # configuration items copied from Qwen
  rotary_pct: 1.0
  rotary_emb_base: 10000
  kv_channels: 128
